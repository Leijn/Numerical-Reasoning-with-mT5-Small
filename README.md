# Numerical Reasoning with mT5-Small: Addressing Cloze Test Challenges
This project focused on Subtask 2 of Se- mEval 2024 Task 7: Comprehension of Numerals in Text, using the NQuAD dataset (Chen et al., 2021), which is in traditional Chinese and consists of over 70,000 questions. To address the task, we fine-tuned the mT5-Small model with a structured instruction template, achieving an accuracy of 87.00%, outperforming several BERT-based methods while closely approaching the top-performing T5-based model (Chen et al., 2024a). Despite computational limitations, our approach demonstrated the effectiveness of instruction fine-tuning in enabling the model to reason about numerical information. Furthermore, we introduced specific evaluation rules to handle edge cases in the dataset, ensuring the validity and reliability of the modelâ€™s outputs. These findings highlight the potential of multilingual models in handling complex numerical reasoning tasks efficiently, providing a foundation for future research.

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LT_fLmQG9Qpp",
    "outputId": "ef13ed65-532e-49a3-de50-a54ae244493f"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HeZDoeeQ9RR-",
    "outputId": "53905e6d-737c-46ac-e3bc-d08fb68f00d7"
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "28HdnBhyV1_v"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "0f2c961dc9ff47b9b60b0aafaa154fb1",
      "0fe1664205e54ec1a175a901082bf4dd",
      "191250ff9084438cbc2d1d94b32b69a5",
      "1f965bb8704942a0acc05451531afb03",
      "352702a23f85403388cf63698253b615",
      "2e6261f8719749a7ad616ad443637c6c",
      "5a25eda208d24ca9aa69f6645d6650a3",
      "c0a8e458c4cd4c368c6170ceb9ae97fe",
      "7f608d852075402eadee4b678e078710",
      "bd967611386342e09f0e10cea9b63792",
      "f507e46471ad4aeba1f31a28ffec7e54",
      "e19c863b59a641efaf93704631cc110a",
      "73f10b70aa944de7aa0b45cf6931dec3",
      "2271d95c2bb141cd97348b9cb0394910",
      "ccf3755ad1394c06b960cfa4e12c86a9",
      "11676da096b543a2a7a9a720edcb76d4",
      "91026ac479bc45938a6cd9d3ddb10a7b",
      "96ef10427f824a5e8575728fa1e686cc",
      "f303b868ed254b3bb65ae79560c48db7",
      "b7e7dc6214db479b977a58f2c38b9e6b",
      "40e4e1f7ab0d4bb9b34aa98e11e66088",
      "a62411db10c448788e73137690c40470"
     ]
    },
    "id": "xXYCc5lPPlci",
    "outputId": "8cb05148-a701-4060-d3d6-3a67f82df20f"
   },
   "outputs": [],
   "source": [
    "# Load the training and validation sets\n",
    "train_data = load_dataset(\"json\", data_files={\"train\": \"/content/drive/MyDrive/NQuAD_train.json\"})\n",
    "eval_data = load_dataset(\"json\", data_files={\"test\": \"/content/drive/MyDrive/NQuAD_test.json\"})\n",
    "\n",
    "train_dataset = train_data[\"train\"]\n",
    "eval_dataset = eval_data[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zKVFbT3QWB5Q",
    "outputId": "e59b7b3f-4796-4037-cee7-b41e896ae05c"
   },
   "outputs": [],
   "source": [
    "print(f\"Size of the training set: {len(train_dataset)}\")\n",
    "print(f\"Size of the testing set: {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358,
     "referenced_widgets": [
      "14f47ea3d0144a50adfa45ed2273b85e",
      "ba9845477a6b4bb088dbcf83a4ac74c2",
      "6aae7bd25ac34791b0100dfee83d9801",
      "2da7a466f28c4af795064d80c0f8a800",
      "d5cdb38b48084218ab1f64d40f2fab71",
      "a4f5da3c767249caa94464c2b6a05373",
      "69442007961e44f7bbfc0c281cdcce21",
      "db1a5ffd734b495fb698ddea3cd625d6",
      "4cb749feb9a04646a58254b9ef77afbd",
      "6d281aff2cbd41dabeb56fa1d1adae57",
      "ed1dcf1a5ac442f2b2dbe3eb7769eb2a",
      "0d5e65b588db4a9ea6b43724cb6cdfa8",
      "a3e3a8d124b7427d8df3a387d7c871ad",
      "53dcc6dcf3f641d6bd169275310afff5",
      "0fef6a0d66794396a09f2b7d8dce9838",
      "daaf0b2d1e274925a6e81f790a2a8a47",
      "97d5029aba3e4c79933ec8edd67b95f7",
      "992207991db545179b51af8e0a01e74b",
      "8767113e446240f69aa067d6f4df2784",
      "d874fc7e38094d0690c52b9c9b8f4352",
      "638b215dbcfe4cf59fce989736910ca2",
      "43316ab5c0e34562a4dd17ff905b5836",
      "b8f216acc58347a78b0e5729bfbc298c",
      "e53b5ee287084cefae2d51f761ce3ab8",
      "a38b4cd6db964c0b86504029a0a0abdf",
      "55e0c2a71bf74b83af21aa98d28e8b20",
      "0a3ffaad5ef241ce8e3c7e17d3fc229b",
      "d451cf7a9c794309bad4de94cda68ed0",
      "36481596d73c4d73ad763d2ee2d0ea03",
      "a7f56e125e5046f193cdf32a8326ce2a",
      "44189e4f39bd4d919189cc332c7576ed",
      "985bc16b56a240e98cb52f31699af725",
      "2d1607ab45a34c0380e0d8b1710c9e5a",
      "858faa32b4a34ee2a979f5d0ee26f046",
      "36885e51b3164ffb96311b56c323e67e",
      "aedf639ba5424d689938014eb3e36126",
      "08ccfddd384d41db806dc3bb6abd1745",
      "63ec12219eab42b7aa45164aac62ab8f",
      "c6682960504d4d16921be1fe00a71678",
      "4a77683a973b43b7b5d587ce865c8d9e",
      "bcede09499ff4f9da7b3c9c3fa316ee9",
      "1aa8e71b9c5b4b469c16c21b23e1a5ab",
      "ef541df171b042e4be1a66f72fcfc236",
      "7b92a181b5cd430c963f1271f55b00ad",
      "60c2ff1cd2e045bfa77faf14adc99b56",
      "e1a3b436ab9d4d118c1f776af9e06438",
      "3f91e158ba7a4890ae78a685deb26263",
      "4dac6ff7b3fa4decae867ee97afb10c3",
      "66274f2e2d454feaba889249793a3739",
      "89bcd1c4177848a0b9c880e17a05e3e6",
      "8a1bb5a22e6c429ba79dcabcb97b9d54",
      "91862a72625e48c2a4dc8bef446327fa",
      "bf8108159c33444fb4134d3d96c5fd2c",
      "f6f8002c4b0a4ba4a6a69449783ab471",
      "b2c4bd720ca24ead80d52a677a94b826",
      "27fbf4dafbf541929a3ae6c6990f29ec",
      "9eeb7952aeac4a5e96126b565516fa6c",
      "4391c86577fa4707ad8f4fccd658f325",
      "2f672dff87bf4ef5936f62bb1b9cab31",
      "6b9f57f5a39c48a18cb20283f18f04aa",
      "2de533eb48804211a36c6ed08b7dc0d9",
      "dd362c1f3aa44fccbac59e8b24a4700f",
      "28d02afcc3ee47cda167a6c2f5259879",
      "84b409c0ace54283854a2b13eb9bf400",
      "01d9903dbbab476087818405c97ab106",
      "ddf49a3bb2944445af0cc22d3b25a58b"
     ]
    },
    "id": "LzlEqAyscrGy",
    "outputId": "7981ac24-8259-43fc-f312-192df03b3ec4"
   },
   "outputs": [],
   "source": [
    "# mT5 tokenizer and model\n",
    "model_name = \"google/mt5-small\"\n",
    "tokenizer = MT5Tokenizer.from_pretrained(model_name)\n",
    "model = MT5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UGFOoa9ZVuhV"
   },
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "def preprocess_function(examples):\n",
    "    inputs = [\n",
    "        f'''\n",
    "        Choose a correct answer to the following questions.\n",
    "        Context: {c}\n",
    "        Question: {q}\n",
    "        Options: {opts}\n",
    "        Answer: '''\n",
    "        for c, q, opts in zip(\n",
    "            examples[\"sentences_containing_the_numeral_in_answer_options\"],\n",
    "            examples[\"question_stem\"],\n",
    "            examples[\"answer_options\"]\n",
    "        )\n",
    "    ]\n",
    "    targets = [str(ans) for ans in examples[\"target_num\"]]\n",
    "\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding=True)\n",
    "    labels = tokenizer(text_target=targets, max_length=2, truncation=True, padding=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = np.array(labels[\"input_ids\"], dtype=np.int64)\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "apN_Ij4-WO2S",
    "outputId": "2a984b53-d4a0-4a6e-fa26-1928c0caa352"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Input Example\n",
    "example_data = {\n",
    "    \"sentences_containing_the_numeral_in_answer_options\": [\n",
    "        \"隨著iPhone 5、三星Galaxy S4(見附圖)等高階智慧型手機銷售不如預期，也讓市場轉而看好中、低階機種的成長力道，並對晶圓代工40奈米製程的需求轉趨樂觀。\"\n",
    "    ],\n",
    "    \"question_stem\": [\n",
    "        \"___nm需求趨緊俏，大摩大升聯電/中芯目標價\"\n",
    "    ],\n",
    "    \"answer_options\": [\n",
    "        [22, 28, 30, 40]\n",
    "    ],\n",
    "    \"target_num\": [\n",
    "        40\n",
    "    ]\n",
    "}\n",
    "\n",
    "processed_data = preprocess_function(example_data)\n",
    "\n",
    "print(\"Input Example:\", tokenizer.decode(processed_data[\"input_ids\"][0]))\n",
    "print(\"Target Example:\", tokenizer.decode(processed_data[\"labels\"][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "475b3ea9515247ef9606a6ec4c3a202a",
      "5502f6f5bff14b218f4fb9689a4a2916",
      "71202556b21048bab83167221799a46d",
      "4783e6d19d3e4987ad5b1836798c57af",
      "4744eaccbd9f4d95a5840e47b806ea03",
      "15131d1b0c9b4bdb89c28ef16d7c609e",
      "6ccc279332e84121957ecb24207024bf",
      "cb396f6d49d5430785488be321ae9849",
      "afec153aaf454c4aaed93c57096ab3fe",
      "efd168b7ce8a48e3a6e849cd7643c52e",
      "6e99f41b59754d64a29b5b9db748d42a",
      "3d8a2e7cb24b470d83c4f897cead9721",
      "1af44666c07a45c4a338b5de6e5dff18",
      "5ff9c45808994506b6796d7315dbaaf2",
      "ea96ba6b8f414858bbb54d77d5d3422b",
      "16817066d47947599c745921672e3214",
      "6b27a917fb654403b9eaaf955e2e1da1",
      "10ad549c17334ea0bc5d2581e2a26f56",
      "25e492d3e0f148de8497bf0a00dc91f2",
      "ea6d20834aad47d6996d0c920d610003",
      "cdbb781fd58e484688e7cafb1a8f0daa",
      "ce52acacc7dc49db8ff5f06164ef1967"
     ]
    },
    "id": "dlk7kWkWV4lY",
    "outputId": "6b1ee54a-eada-46da-9666-9a1f20d2b3bd"
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "eval_dataset = eval_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Remove other columns\n",
    "columns_to_remove = [\"news_article\", \"question_stem\", \"answer_options\", \"ans\", \"target_num\", \"sentences_containing_the_numeral_in_answer_options\"]\n",
    "train_dataset = train_dataset.remove_columns(columns_to_remove)\n",
    "eval_dataset = eval_dataset.remove_columns(columns_to_remove)\n",
    "\n",
    "# Convert to PyTorch format\n",
    "train_dataset.set_format(\"torch\")\n",
    "eval_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 990
    },
    "id": "mTuppw7fiL7Y",
    "outputId": "67376cf6-2b14-4522-8dc7-b62791b61a03"
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, EarlyStoppingCallback\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Define the data collator\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    padding=True,\n",
    "    label_pad_token_id=-100\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    run_name=\"mt5-finetuning-run1\",\n",
    "    report_to=\"none\",\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    logging_steps=100,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_first_step=True,\n",
    "    logging_nan_inf_filter=False,\n",
    "    learning_rate=3e-4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    gradient_accumulation_steps=4,\n",
    "    predict_with_generate=False,\n",
    ")\n",
    "\n",
    "# Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=data_collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],  # Add early stopping\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rPeMLMmJ9IWn"
   },
   "outputs": [],
   "source": [
    "################# Model Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P5TpMh1KiaSV",
    "outputId": "08ff0b8e-2866-4b27-ab42-67c747220832"
   },
   "outputs": [],
   "source": [
    "# Save the model to the specified directory\n",
    "output_dir = \"./mt5_finetuned_model_new\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4xL6seEkJY8",
    "outputId": "6ccaf637-2d20-48c9-eca2-5d8b1f53e6ca"
   },
   "outputs": [],
   "source": [
    "# Save to Google Drive\n",
    "import shutil\n",
    "shutil.move(output_dir, \"/content/drive/MyDrive/mt5_finetuned_model\")\n",
    "\n",
    "print(\"Model saved to Google Drive!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4yfhbQhDXfYv"
   },
   "outputs": [],
   "source": [
    "#######################Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "093f0ad2c85c41dc9f241e7108082a63",
      "2c798adb637a47119167244c582b7124",
      "ff6517268da2479696025dd4e5ecb763",
      "94d88ddc54484026858997163e79396a",
      "bd36fc58c7e943b58b78896ad673131c",
      "ab5a37dae2de4f91a8f879f657b5d544",
      "4c85c7f374c4422e89a69d333e61f49d",
      "2ea1ec4955dd44a9ab2be4446634f0c0",
      "3ec3e734693b40fabe3e0ac8e89aa206",
      "9c48353d486a4d878fe9246d3ae48a47",
      "f2b47041fe394a118409ec9c308bba7e"
     ]
    },
    "id": "cmtiJ8Mp2WoR",
    "outputId": "d8eacfe7-b9a6-4237-e46c-e360b0cc6788"
   },
   "outputs": [],
   "source": [
    "# Evaluation dataset with options column\n",
    "eval_dataset_new = eval_data[\"test\"]\n",
    "eval_dataset_new = eval_dataset_new.map(preprocess_function, batched=True)\n",
    "columns_remove = [\"news_article\", \"question_stem\", \"ans\", \"target_num\", \"sentences_containing_the_numeral_in_answer_options\"]\n",
    "eval_dataset_new = eval_dataset_new.remove_columns(columns_remove)\n",
    "eval_dataset_new.set_format(\"torch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m25ikbMy6BBi"
   },
   "outputs": [],
   "source": [
    "def evaluate_samples(dataset, model, tokenizer, max_length=2, max_retries=3):\n",
    "    results = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for sample in dataset:\n",
    "            # Retrieve and process input, target, and options\n",
    "            input_ids = sample[\"input_ids\"].unsqueeze(0).to(model.device)  # Add batch dimension\n",
    "            labels = sample[\"labels\"].unsqueeze(0).to(model.device)     # Target values\n",
    "            target_text = tokenizer.decode(labels[0], skip_special_tokens=True).strip()  # Decode target\n",
    "            options = sample[\"answer_options\"]  # Options provided in the dataset\n",
    "\n",
    "            # Skip this sample if the target is not in the options\n",
    "            if target_text not in options:\n",
    "                continue\n",
    "\n",
    "            predicted_text = \"\"\n",
    "            retries = 0\n",
    "\n",
    "            # Generate predictions and handle empty predictions\n",
    "            while not predicted_text.strip() and retries < max_retries:\n",
    "                outputs = model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    max_length=max_length,\n",
    "                    early_stopping=False\n",
    "                )\n",
    "                predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "                retries += 1\n",
    "\n",
    "            # Ensure the prediction is in the options\n",
    "            if predicted_text not in options:\n",
    "                # Find the closest match among the options\n",
    "                corrected_prediction = min(\n",
    "                    options, key=lambda opt: len(set(opt) ^ set(predicted_text))\n",
    "                )\n",
    "            else:\n",
    "                corrected_prediction = predicted_text\n",
    "\n",
    "            # Append results if prediction is valid\n",
    "            if corrected_prediction:\n",
    "                results.append({\n",
    "                    \"input\": tokenizer.decode(input_ids[0], skip_special_tokens=True),\n",
    "                    \"target\": target_text,\n",
    "                    \"prediction\": predicted_text,\n",
    "                    \"corrected_prediction\": corrected_prediction,\n",
    "                })\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OS00mXLL3sH"
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(results):\n",
    "    correct = 0\n",
    "    total = len(results)\n",
    "\n",
    "    for result in results:\n",
    "        # Compare whether target and prediction are equal\n",
    "        if result[\"target\"] == result[\"prediction\"]:\n",
    "            correct += 1\n",
    "\n",
    "    # Ensure total is not 0\n",
    "    accuracy = correct / total if total > 0 else 0.0\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Cz-AcQWz4-F"
   },
   "outputs": [],
   "source": [
    "# from torch.utils.data import Subset\n",
    "\n",
    "#\n",
    "# subset_indices = list(range(50))  # Select the first 50 data\n",
    "# subset_eval_dataset = Subset(eval_dataset_new, subset_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "19YybRsoL5tJ",
    "outputId": "15679a7c-0364-40d7-b552-4a821552cebc"
   },
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "results = evaluate_samples(eval_dataset_new, model, tokenizer)\n",
    "# Compute Accuracy\n",
    "accuracy = calculate_accuracy(results)\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ckbcZDcd3og"
   },
   "outputs": [],
   "source": [
    "#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CtWyriQuvHoe",
    "outputId": "f1c71320-6af6-4fb9-d06f-4bc777689429"
   },
   "outputs": [],
   "source": [
    "# Check\n",
    "for i, result in enumerate(results[:50]):\n",
    "    print(f\"Sample {i + 1}:\")\n",
    "    print(f\"Input: {result['input']}\")\n",
    "    print(f\"Target: {result['target']}\")\n",
    "    print(f\"Prediction: {result['prediction']}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
